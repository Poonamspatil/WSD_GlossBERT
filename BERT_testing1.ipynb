{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import argparse\n",
    "from tokenization import BertTokenizer\n",
    "from modeling import BertForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import sys\n",
    "from nltk.corpus import wordnet\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_context_gloss_pairs_through_nltk(input, target_start_id, target_end_id, gloss_dict):\n",
    "    \"\"\"\n",
    "    construct context gloss pairs like sent_cls_ws\n",
    "    :param input: str, a sentence\n",
    "    :param target_start_id: int\n",
    "    :param target_end_id: int\n",
    "    :param lemma: lemma of the target word\n",
    "    :return: candidate lists\n",
    "    \"\"\"\n",
    "    sent = input.split(\" \")\n",
    "    assert 0 <= target_start_id and target_start_id < target_end_id  and target_end_id <= len(sent)\n",
    "    target = \" \".join(sent[target_start_id:target_end_id])\n",
    "    if len(sent) > target_end_id:\n",
    "        sent = sent[:target_start_id] + ['\"'] + sent[target_start_id:target_end_id] + ['\"'] + sent[target_end_id:]\n",
    "    else:\n",
    "        sent = sent[:target_start_id] + ['\"'] + sent[target_start_id:target_end_id] + ['\"']\n",
    "\n",
    "    sent = \" \".join(sent)\n",
    "\n",
    "    candidate = []\n",
    "    for gloss_def in gloss_dict['synsets']:\n",
    "        candidate.append((sent, f\"{target} : {gloss_def}\", target, gloss_def))\n",
    "    #syns = wordnet.synsets(target)\n",
    "    #for syn in syns:\n",
    "    #    gloss = syn.definition()\n",
    "    #    candidate.append((sent, f\"{target} : {gloss}\", target, gloss))\n",
    "\n",
    "    assert len(candidate) != 0, f'there is no candidate sense of \"{target}\" in WordNet, please check'\n",
    "    #print(f'there are {len(candidate)} candidate senses of \"{target}\"')\n",
    "\n",
    "\n",
    "    return candidate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_features(candidate, tokenizer, max_seq_length=512):\n",
    "\n",
    "    candidate_results = []\n",
    "    features = []\n",
    "    for item in candidate:\n",
    "        text_a = item[0] # sentence\n",
    "        text_b = item[1] # gloss\n",
    "        candidate_results.append((item[-2], item[-1])) # (target, gloss)\n",
    "\n",
    "\n",
    "        tokens_a = tokenizer.tokenize(text_a)\n",
    "        tokens_b = tokenizer.tokenize(text_b)\n",
    "        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "        tokens += tokens_b + [\"[SEP]\"]\n",
    "        segment_ids += [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(input_ids=input_ids,\n",
    "                          input_mask=input_mask,\n",
    "                          segment_ids=segment_ids))\n",
    "\n",
    "\n",
    "    return features, candidate_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(input, target_start_id, target_end_id, gloss_dict, args):\n",
    "\n",
    "    sent = input.split(\" \")\n",
    "    assert 0 <= target_start_id and target_start_id < target_end_id  and target_end_id <= len(sent)\n",
    "    target = \" \".join(sent[target_start_id:target_end_id])\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "\n",
    "\n",
    "    label_list = [\"0\", \"1\"]\n",
    "    num_labels = len(label_list)\n",
    "    tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=True)\n",
    "    model = BertForSequenceClassification.from_pretrained(args.bert_model,\n",
    "                                                          num_labels=num_labels)\n",
    "    model.to(device)\n",
    "\n",
    "    print(f\"input: {input}\\ntarget: {target}\")\n",
    "\n",
    "    examples = construct_context_gloss_pairs_through_nltk(input, target_start_id, target_end_id, gloss_dict)\n",
    "    eval_features, candidate_results = convert_to_features(examples, tokenizer)\n",
    "    input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "    input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "    segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_mask, labels=None)\n",
    "    logits_ = F.softmax(logits, dim=-1)\n",
    "    logits_ = logits_.detach().cpu().numpy()\n",
    "    output = np.argmax(logits_, axis=0)[1]\n",
    "    print(f\"results:\\ngloss: {candidate_results[output][1]}\")\n",
    "    return candidate_results[output][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(input_string, punc1, punc2):\n",
    "    \"\"\"\n",
    "    remove/ replace unnecessary punctuations from ontext sentence\n",
    "    \n",
    "    :param input_string: strs, input string/ token\n",
    "    :param punc1: str, punctuations to replace\n",
    "    :param punc2: str, punctuations to remove\n",
    "    :return input_string:str\n",
    "    \"\"\"\n",
    "    \n",
    "    for ele in input_string:  \n",
    "        if ele in punc1:  \n",
    "            input_string = input_string.replace(ele, \" \")\n",
    "        if ele in punc2:  \n",
    "            input_string = input_string.replace(ele, \"\")\n",
    "    return input_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### to get context sentense from paragraph\n",
    "def get_context_sentence(word, para):\n",
    "    \"\"\"\n",
    "    Get context sentense for the given word and paragraph\n",
    "    \n",
    "    :param input: str, a sentence\n",
    "    :param target_start_id: int\n",
    "    :param target_end_id: int\n",
    "    :param lemma: lemma of the target word\n",
    "    :return: candidate lists \n",
    "        word (str) target word\n",
    "        sent (str) context sentence\n",
    "        target_start_id (int), starting index of target word\n",
    "        target_end_id (int), end index of target word\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(para)\n",
    "    sentence_lst=[sent.lower() for sent in sentences if word.lower() in sent.lower()]\n",
    "    \n",
    "    sent = sentence_lst[0]\n",
    "    \n",
    "    word = remove_punctuation(word, punc1, punc2)\n",
    "    sent = remove_punctuation(sent, punc1, punc2)\n",
    "    print('sentence: ', sent)\n",
    "    print('word:', word)\n",
    "\n",
    "    target_tokens = word_tokenize(word)   # create word tokens for input target string\n",
    "    sent_tokens = word_tokenize(sent)     # create word tokens for context sentence\n",
    "\n",
    "    target_start_id = sent_tokens.index(target_tokens[0])\n",
    "    target_end_id = sent_tokens.index(target_tokens[-1]) + 1     # +1 to get correct ending index\n",
    "    \n",
    "    # get POS tag for target word\n",
    "    tagged_sent = nltk.pos_tag(sent_tokens)           # get POS tags of tokens in the sentence\n",
    "    target_penn_pos = tagged_sent[target_start_id][1] # get POS tag in Penntree notation for target word\n",
    "    target_mb_pos = get_pos_MB(target_penn_pos)       # get POS tag in meriam webster notation for target word\n",
    "    \n",
    "    print('target_start_id:',target_start_id)\n",
    "    print('target_end_id:', target_end_id)\n",
    "    print('target_penn_pos', target_penn_pos)\n",
    "    print('target_mb_pos:', target_mb_pos)\n",
    "    \n",
    "    return word, sent, target_start_id, target_end_id, target_mb_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_MB(pos_penn_tree):\n",
    "    \"\"\"\n",
    "    Description: Function takes part-of-speech (POS) for target word using Penn Treebank notation \n",
    "                and returns part of speech in the form of Merriam webster notation (i.e. 'fl' part in json input)\n",
    "    \n",
    "    Input: \n",
    "        pos_penn_tree: (str), part of speech (POS) for target word in Penn Treebank notation form\n",
    "    \n",
    "    Output: \n",
    "        pos_MB: (str), part of speech to match with 'fl' in Merriam Webster dictionary\n",
    "    \"\"\"\n",
    "    pos_MB = [pos for pos, item_list in pos_tag.items() if pos_penn_tree in item_list]\n",
    "    return pos_MB[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2020 12:01:39 - INFO - tokenization -   loading vocabulary file ./Sent_CLS_WS/vocab.txt\n",
      "12/06/2020 12:01:39 - INFO - modeling -   loading archive file ./Sent_CLS_WS\n",
      "12/06/2020 12:01:39 - INFO - modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target word: manslaughter\n",
      "sentence:  the shooter was charged with manslaughter\n",
      "word: manslaughter\n",
      "target_start_id: 5\n",
      "target_end_id: 6\n",
      "target_penn_pos NN\n",
      "target_mb_pos: noun\n",
      "input: the shooter was charged with manslaughter\n",
      "target: manslaughter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2020 12:01:44 - INFO - tokenization -   loading vocabulary file ./Sent_CLS_WS/vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results:\n",
      "gloss: the crime of killing a person without intending to do so\n",
      "target word: bludgeoned\n",
      "sentence:  there was vincent chin in detroit who in 1982 was bludgeoned to death with a baseball bat a week before his wedding\n",
      "word: bludgeoned\n",
      "target_start_id: 10\n",
      "target_end_id: 11\n",
      "target_penn_pos VBN\n",
      "target_mb_pos: verb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2020 12:01:44 - INFO - modeling -   loading archive file ./Sent_CLS_WS\n",
      "12/06/2020 12:01:44 - INFO - modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: there was vincent chin in detroit who in 1982 was bludgeoned to death with a baseball bat a week before his wedding\n",
      "target: bludgeoned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2020 12:01:50 - INFO - tokenization -   loading vocabulary file ./Sent_CLS_WS/vocab.txt\n",
      "12/06/2020 12:01:50 - INFO - modeling -   loading archive file ./Sent_CLS_WS\n",
      "12/06/2020 12:01:50 - INFO - modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results:\n",
      "gloss: to hit with heavy impact\n",
      "target word: envisioned\n",
      "sentence:  i envisioned my students galvanized as i had been in eighth grade by martin luther king jrs letter from birmingham jail and mesmerized as i had been in high school by malcolm xs autobiography\n",
      "word: envisioned\n",
      "target_start_id: 1\n",
      "target_end_id: 2\n",
      "target_penn_pos VBD\n",
      "target_mb_pos: verb\n",
      "input: i envisioned my students galvanized as i had been in eighth grade by martin luther king jrs letter from birmingham jail and mesmerized as i had been in high school by malcolm xs autobiography\n",
      "target: envisioned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2020 12:01:56 - INFO - tokenization -   loading vocabulary file ./Sent_CLS_WS/vocab.txt\n",
      "12/06/2020 12:01:56 - INFO - modeling -   loading archive file ./Sent_CLS_WS\n",
      "12/06/2020 12:01:56 - INFO - modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results:\n",
      "gloss: to picture to oneself\n",
      "target word: bucket\n",
      "sentence:  they stacked foot high collections of michael jackson and joan baez records in the living room voted dutifully never missing an election and brought home for dinner the occasional bucket of fried chicken\n",
      "word: bucket\n",
      "target_start_id: 29\n",
      "target_end_id: 30\n",
      "target_penn_pos NN\n",
      "target_mb_pos: noun\n",
      "input: they stacked foot high collections of michael jackson and joan baez records in the living room voted dutifully never missing an election and brought home for dinner the occasional bucket of fried chicken\n",
      "target: bucket\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2020 12:02:12 - INFO - tokenization -   loading vocabulary file ./Sent_CLS_WS/vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results:\n",
      "gloss: a typically cylindrical vessel for catching, holding, or carrying liquids or solids\n",
      "target word: acquitted\n",
      "sentence:  he claimed at court that the kid moved in a strange way his lawyer told the jury that the killer was protecting his property and that he was an average joe one of your neighbors someone who liked sugar in his grits he was acquitted\n",
      "word: acquitted\n",
      "target_start_id: 44\n",
      "target_end_id: 45\n",
      "target_penn_pos VBN\n",
      "target_mb_pos: verb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2020 12:02:12 - INFO - modeling -   loading archive file ./Sent_CLS_WS\n",
      "12/06/2020 12:02:12 - INFO - modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: he claimed at court that the kid moved in a strange way his lawyer told the jury that the killer was protecting his property and that he was an average joe one of your neighbors someone who liked sugar in his grits he was acquitted\n",
      "target: acquitted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2020 12:02:17 - INFO - tokenization -   loading vocabulary file ./Sent_CLS_WS/vocab.txt\n",
      "12/06/2020 12:02:17 - INFO - modeling -   loading archive file ./Sent_CLS_WS\n",
      "12/06/2020 12:02:17 - INFO - modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results:\n",
      "gloss: to decide that someone is not guilty of a crime\n",
      "target word: allegation\n",
      "sentence:  i took this allegation literally\n",
      "word: allegation\n",
      "target_start_id: 3\n",
      "target_end_id: 4\n",
      "target_penn_pos NN\n",
      "target_mb_pos: noun\n",
      "input: i took this allegation literally\n",
      "target: allegation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2020 12:02:22 - INFO - tokenization -   loading vocabulary file ./Sent_CLS_WS/vocab.txt\n",
      "12/06/2020 12:02:22 - INFO - modeling -   loading archive file ./Sent_CLS_WS\n",
      "12/06/2020 12:02:22 - INFO - modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results:\n",
      "gloss: the act of alleging something\n",
      "target word: primed\n",
      "sentence:  they effected a clandestine evangelization of a kid primed to be a good disciple\n",
      "word: primed\n",
      "target_start_id: 8\n",
      "target_end_id: 9\n",
      "target_penn_pos VBN\n",
      "target_mb_pos: verb\n",
      "input: they effected a clandestine evangelization of a kid primed to be a good disciple\n",
      "target: primed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2020 12:02:28 - INFO - tokenization -   loading vocabulary file ./Sent_CLS_WS/vocab.txt\n",
      "12/06/2020 12:02:28 - INFO - modeling -   loading archive file ./Sent_CLS_WS\n",
      "12/06/2020 12:02:28 - INFO - modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results:\n",
      "gloss: to make (someone) ready to do something : prepare\n",
      "target word: laden\n",
      "sentence:  education for me became laden with a meaning at once specific and spiritual\n",
      "word: laden\n",
      "target_start_id: 4\n",
      "target_end_id: 5\n",
      "target_penn_pos JJ\n",
      "target_mb_pos: adjective\n",
      "input: education for me became laden with a meaning at once specific and spiritual\n",
      "target: laden\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2020 12:02:33 - INFO - tokenization -   loading vocabulary file ./Sent_CLS_WS/vocab.txt\n",
      "12/06/2020 12:02:33 - INFO - modeling -   loading archive file ./Sent_CLS_WS\n",
      "12/06/2020 12:02:33 - INFO - modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results:\n",
      "gloss: loaded heavily with something : having or carrying a large amount of something —usually + with —often used figuratively\n",
      "target word: rhetoric\n",
      "sentence:  in suburban michigan in the quiet of my bedroom these iconic readings of antiracist rhetoric cast a spell over me\n",
      "word: rhetoric\n",
      "target_start_id: 14\n",
      "target_end_id: 15\n",
      "target_penn_pos NN\n",
      "target_mb_pos: noun\n",
      "input: in suburban michigan in the quiet of my bedroom these iconic readings of antiracist rhetoric cast a spell over me\n",
      "target: rhetoric\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2020 12:02:37 - INFO - tokenization -   loading vocabulary file ./Sent_CLS_WS/vocab.txt\n",
      "12/06/2020 12:02:37 - INFO - modeling -   loading archive file ./Sent_CLS_WS\n",
      "12/06/2020 12:02:37 - INFO - modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results:\n",
      "gloss: the art of speaking or writing effectively: such as\n",
      "target word: essential\n",
      "sentence:  du bois ralph ellison richard wright alice walker maya angelou each of these people seemed as fearless to me as asian americans seemed afraid as essential to american history as we were irrelevant\n",
      "word: essential\n",
      "target_start_id: 25\n",
      "target_end_id: 26\n",
      "target_penn_pos JJ\n",
      "target_mb_pos: adjective\n",
      "input: du bois ralph ellison richard wright alice walker maya angelou each of these people seemed as fearless to me as asian americans seemed afraid as essential to american history as we were irrelevant\n",
      "target: essential\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2020 12:02:41 - INFO - tokenization -   loading vocabulary file ./Sent_CLS_WS/vocab.txt\n",
      "12/06/2020 12:02:41 - INFO - modeling -   loading archive file ./Sent_CLS_WS\n",
      "12/06/2020 12:02:41 - INFO - modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results:\n",
      "gloss: of, relating to, or constituting essence : inherent\n",
      "target word: steeped\n",
      "sentence:  i imagined households steeped in conversation\n",
      "word: steeped\n",
      "target_start_id: 3\n",
      "target_end_id: 4\n",
      "target_penn_pos VBD\n",
      "target_mb_pos: verb\n",
      "input: i imagined households steeped in conversation\n",
      "target: steeped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2020 12:02:46 - INFO - tokenization -   loading vocabulary file ./Sent_CLS_WS/vocab.txt\n",
      "12/06/2020 12:02:46 - INFO - modeling -   loading archive file ./Sent_CLS_WS\n",
      "12/06/2020 12:02:46 - INFO - modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results:\n",
      "gloss: to put (something) in a liquid for a period of time\n",
      "target word: embolden\n",
      "sentence:  did it embolden you\n",
      "word: embolden\n",
      "target_start_id: 2\n",
      "target_end_id: 3\n",
      "target_penn_pos VB\n",
      "target_mb_pos: verb\n",
      "input: did it embolden you\n",
      "target: embolden\n",
      "results:\n",
      "gloss: to impart boldness or courage to : to instill with boldness, courage, or resolution enough to overcome timidity or misgiving\n",
      "target word: from scratch\n",
      "sentence:  i would start from scratch\n",
      "word: from scratch\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-2ea34125073e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'target word:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# get context sentence for target word and index details of target word in context sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_start_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_end_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_context_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paragraph'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m#get_pos_MB()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-e1d0c6f57765>\u001b[0m in \u001b[0;36mget_context_sentence\u001b[0;34m(word, para)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mtagged_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_tokens\u001b[0m\u001b[0;34m)\u001b[0m           \u001b[0;31m# get POS tags of tokens in the sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mtarget_penn_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagged_sent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_start_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# get POS tag in Penntree notation for target word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mtarget_mb_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pos_MB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_penn_pos\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# get POS tag in meriam webster notation for target word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'target_start_id:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_start_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-aa6b59f73ae5>\u001b[0m in \u001b[0;36mget_pos_MB\u001b[0;34m(pos_penn_tree)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \"\"\"\n\u001b[1;32m     12\u001b[0m     \u001b[0mpos_MB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_list\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpos_penn_tree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpos_MB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "if  __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-f')\n",
    "    parser.add_argument(\"--bert_model\", default=\"./Sent_CLS_WS\", type=str)\n",
    "    parser.add_argument(\"--no_cuda\", default=False, action='store_true', help=\"Whether not to use CUDA when available\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    request_file = open('request1.json', 'r')\n",
    "    request_data = request_file.read()\n",
    "    jsonobj = json.loads(request_data)\n",
    "    \n",
    "    #jsonobj_test = jsonobj[:2]\n",
    "    \n",
    "    #* Define global variables *#\n",
    "    # Create a dictionary to as a cross reference for part-of-speech between penn tree and Merriam Webster dictionary\n",
    "    pos_tag = {'verb':['VB','VBD','VBG','VBN','VBP','VBZ'], \n",
    "           'noun': ['NN','NNS','NNP', 'NNPS'], \n",
    "           'adjective': ['JJ','JJR','JJS'],\n",
    "           'adverb':['RB', 'RBR', 'RBS', 'RP'],\n",
    "           'None':['None',''] }\n",
    "\n",
    "    punc1 = '-—'\n",
    "    #punc2 =':.,;“”’()?'\n",
    "    punc2 = '''!()[]{};:'\"’“”\\,<>./?@#$%^&*_~'''\n",
    "    word_id_lst=[]\n",
    "    teacher_vote_lst=[]\n",
    "    ai_vote_lst=[]\n",
    "    word_lst=[]\n",
    "\n",
    "    for item in jsonobj:\n",
    "        word = item['word'].lower()                                          # get target word \n",
    " \n",
    "        print('target word:', word)\n",
    "        # get context sentence for target word and index details of target word in context sentence\n",
    "        word, sent, target_start_id, target_end_id, target_pos = get_context_sentence(word, item['paragraph']) \n",
    "        \n",
    "        #get_pos_MB()\n",
    "\n",
    "        gloss_dict={}\n",
    "        gloss_data =[]\n",
    "        for gloss in item['dictionaryData']:\n",
    "            \n",
    "            if gloss['fl'].lower() == target_pos:\n",
    "                gloss_data.append(gloss['meaning'])\n",
    "            \n",
    "            if gloss['isTeacher'] == 1:\n",
    "                word_id = gloss['word_id']\n",
    "                teacher_vote = gloss['meaning']\n",
    "   \n",
    "        gloss_dict['synsets']=gloss_data\n",
    "        ai_vote = infer(sent, target_start_id, target_end_id, gloss_dict, args)\n",
    "\n",
    "        word_id_lst.append(word_id)\n",
    "        word_lst.append(word)\n",
    "        teacher_vote_lst.append(teacher_vote)\n",
    "        ai_vote_lst.append(ai_vote)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(list(zip(word_id_lst, word_lst,teacher_vote_lst, ai_vote_lst)),columns=['word_id', 'word','teachers_vote', 'AI_vote'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7022058823529411"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(results_df['teachers_vote']==results_df['AI_vote'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sente = 'patricks warrant stated capital murder meaning lethal injection in arkansas and had since been reduced to first degree murder still an overcharge'\n",
    "\n",
    "tokens = word_tokenize(sente)\n",
    "tagged_sent = nltk.pos_tag(tokens)\n",
    "\n",
    "target_start_id = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tag = {'verb':['VB','VBD','VBG','VBN','VBP','VBZ'], \n",
    "               'noun': ['NN','NNS','NNP', 'NNPS'], \n",
    "               'adjective': ['JJ','JJR','JJS'],\n",
    "               'adverb':['RB', 'RBR', 'RBS', 'RP'],\n",
    "               'None':['None',''] }\n",
    "\n",
    "def add_pos(pos_value):\n",
    "    # make a dictionary of possible tags (in progress)\n",
    "    pos_val = [pos for pos, item_list in pos_tag.items() if pos_value in item_list]\n",
    "    return pos_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'JJ'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sent[target_start_id][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adjective']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_pos(tagged_sent[target_start_id][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('from scratch', 'NN')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gfg = ['from scratch']\n",
    "nltk.pos_tag(gfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('patricks', 'NNS'),\n",
       " ('warrant', 'VBP'),\n",
       " ('stated', 'VBN'),\n",
       " ('capital', 'NN'),\n",
       " ('murder', 'NN'),\n",
       " ('meaning', 'NN'),\n",
       " ('lethal', 'JJ'),\n",
       " ('injection', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('arkansas', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('had', 'VBD'),\n",
       " ('since', 'IN'),\n",
       " ('been', 'VBN'),\n",
       " ('reduced', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('first', 'JJ'),\n",
       " ('degree', 'NN'),\n",
       " ('murder', 'NN'),\n",
       " ('still', 'RB'),\n",
       " ('an', 'DT'),\n",
       " ('overcharge', 'NN')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-69d8a18c1b5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtagged_sent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lethal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "tagged_sent['lethal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
